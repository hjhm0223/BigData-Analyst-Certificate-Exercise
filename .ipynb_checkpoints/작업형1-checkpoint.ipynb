{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92185395",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      0       1\n",
      "0             alabaster  0.7.12\n",
      "1       anaconda-client   1.7.2\n",
      "2    anaconda-navigator   2.0.3\n",
      "3      anaconda-project   0.9.1\n",
      "4                 anyio   2.2.0\n",
      "..                  ...     ...\n",
      "250                yapf  0.31.0\n",
      "251                zict   2.0.0\n",
      "252                zipp   3.4.1\n",
      "253          zope.event   4.5.0\n",
      "254      zope.interface   5.3.0\n",
      "\n",
      "[255 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources \n",
    "import pandas as pd\n",
    "OutputDataSet = pd.DataFrame(sorted([(i.key, i.version) for i in pkg_resources.working_set])) \n",
    "print(OutputDataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb2866",
   "metadata": {},
   "source": [
    "# Python Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61b8f48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__SKLEARN_SETUP__', '__all__', '__builtins__', '__cached__', '__check_build', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_config', '_distributor_init', 'base', 'clone', 'config_context', 'exceptions', 'get_config', 'logger', 'logging', 'os', 'random', 'set_config', 'setup_module', 'show_versions', 'sys', 'utils']\n",
      "['Binarizer', 'FunctionTransformer', 'KBinsDiscretizer', 'KernelCenterer', 'LabelBinarizer', 'LabelEncoder', 'MaxAbsScaler', 'MinMaxScaler', 'MultiLabelBinarizer', 'Normalizer', 'OneHotEncoder', 'OrdinalEncoder', 'PolynomialFeatures', 'PowerTransformer', 'QuantileTransformer', 'RobustScaler', 'StandardScaler', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_csr_polynomial_expansion', '_data', '_discretization', '_encoders', '_function_transformer', '_label', 'add_dummy_feature', 'binarize', 'label_binarize', 'maxabs_scale', 'minmax_scale', 'normalize', 'power_transform', 'quantile_transform', 'robust_scale', 'scale']\n",
      "Help on class StandardScaler in module sklearn.preprocessing._data:\n",
      "\n",
      "class StandardScaler(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  StandardScaler(*, copy=True, with_mean=True, with_std=True)\n",
      " |  \n",
      " |  Standardize features by removing the mean and scaling to unit variance\n",
      " |  \n",
      " |  The standard score of a sample `x` is calculated as:\n",
      " |  \n",
      " |      z = (x - u) / s\n",
      " |  \n",
      " |  where `u` is the mean of the training samples or zero if `with_mean=False`,\n",
      " |  and `s` is the standard deviation of the training samples or one if\n",
      " |  `with_std=False`.\n",
      " |  \n",
      " |  Centering and scaling happen independently on each feature by computing\n",
      " |  the relevant statistics on the samples in the training set. Mean and\n",
      " |  standard deviation are then stored to be used on later data using\n",
      " |  :meth:`transform`.\n",
      " |  \n",
      " |  Standardization of a dataset is a common requirement for many\n",
      " |  machine learning estimators: they might behave badly if the\n",
      " |  individual features do not more or less look like standard normally\n",
      " |  distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
      " |  \n",
      " |  For instance many elements used in the objective function of\n",
      " |  a learning algorithm (such as the RBF kernel of Support Vector\n",
      " |  Machines or the L1 and L2 regularizers of linear models) assume that\n",
      " |  all features are centered around 0 and have variance in the same\n",
      " |  order. If a feature has a variance that is orders of magnitude larger\n",
      " |  that others, it might dominate the objective function and make the\n",
      " |  estimator unable to learn from other features correctly as expected.\n",
      " |  \n",
      " |  This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
      " |  `with_mean=False` to avoid breaking the sparsity structure of the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  copy : bool, default=True\n",
      " |      If False, try to avoid a copy and do inplace scaling instead.\n",
      " |      This is not guaranteed to always work inplace; e.g. if the data is\n",
      " |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      " |      returned.\n",
      " |  \n",
      " |  with_mean : bool, default=True\n",
      " |      If True, center the data before scaling.\n",
      " |      This does not work (and will raise an exception) when attempted on\n",
      " |      sparse matrices, because centering them entails building a dense\n",
      " |      matrix which in common use cases is likely to be too large to fit in\n",
      " |      memory.\n",
      " |  \n",
      " |  with_std : bool, default=True\n",
      " |      If True, scale the data to unit variance (or equivalently,\n",
      " |      unit standard deviation).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  scale_ : ndarray of shape (n_features,) or None\n",
      " |      Per feature relative scaling of the data to achieve zero mean and unit\n",
      " |      variance. Generally this is calculated using `np.sqrt(var_)`. If a\n",
      " |      variance is zero, we can't achieve unit variance, and the data is left\n",
      " |      as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n",
      " |      when `with_std=False`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_*\n",
      " |  \n",
      " |  mean_ : ndarray of shape (n_features,) or None\n",
      " |      The mean value for each feature in the training set.\n",
      " |      Equal to ``None`` when ``with_mean=False``.\n",
      " |  \n",
      " |  var_ : ndarray of shape (n_features,) or None\n",
      " |      The variance for each feature in the training set. Used to compute\n",
      " |      `scale_`. Equal to ``None`` when ``with_std=False``.\n",
      " |  \n",
      " |  n_samples_seen_ : int or ndarray of shape (n_features,)\n",
      " |      The number of samples processed by the estimator for each feature.\n",
      " |      If there are no missing samples, the ``n_samples_seen`` will be an\n",
      " |      integer, otherwise it will be an array of dtype int. If\n",
      " |      `sample_weights` are used it will be a float (if no missing data)\n",
      " |      or an array of dtype float that sums the weights seen so far.\n",
      " |      Will be reset on new calls to fit, but increments across\n",
      " |      ``partial_fit`` calls.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
      " |  >>> scaler = StandardScaler()\n",
      " |  >>> print(scaler.fit(data))\n",
      " |  StandardScaler()\n",
      " |  >>> print(scaler.mean_)\n",
      " |  [0.5 0.5]\n",
      " |  >>> print(scaler.transform(data))\n",
      " |  [[-1. -1.]\n",
      " |   [-1. -1.]\n",
      " |   [ 1.  1.]\n",
      " |   [ 1.  1.]]\n",
      " |  >>> print(scaler.transform([[2, 2]]))\n",
      " |  [[3. 3.]]\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  scale : Equivalent function without the estimator API.\n",
      " |  \n",
      " |  :class:`~sklearn.decomposition.PCA` : Further removes the linear\n",
      " |      correlation across features with 'whiten=True'.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      " |  transform.\n",
      " |  \n",
      " |  We use a biased estimator for the standard deviation, equivalent to\n",
      " |  `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
      " |  affect model performance.\n",
      " |  \n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StandardScaler\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, copy=True, with_mean=True, with_std=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None, sample_weight=None)\n",
      " |      Compute the mean and std to be used for later scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Individual weights for each sample.\n",
      " |      \n",
      " |          .. versionadded:: 0.24\n",
      " |             parameter *sample_weight* support to StandardScaler.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  inverse_transform(self, X, copy=None)\n",
      " |      Scale back the data to the original representation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data used to scale along the features axis.\n",
      " |      copy : bool, default=None\n",
      " |          Copy the input X or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  partial_fit(self, X, y=None, sample_weight=None)\n",
      " |      Online computation of mean and std on X for later scaling.\n",
      " |      \n",
      " |      All of X is processed as a single batch. This is intended for cases\n",
      " |      when :meth:`fit` is not feasible due to very large number of\n",
      " |      `n_samples` or because X is read from a continuous stream.\n",
      " |      \n",
      " |      The algorithm for incremental mean and std is given in Equation 1.5a,b\n",
      " |      in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n",
      " |      for computing the sample variance: Analysis and recommendations.\"\n",
      " |      The American Statistician 37.3 (1983): 242-247:\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Individual weights for each sample.\n",
      " |      \n",
      " |          .. versionadded:: 0.24\n",
      " |             parameter *sample_weight* support to StandardScaler.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  transform(self, X, copy=None)\n",
      " |      Perform standardization by centering and scaling\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix of shape (n_samples, n_features)\n",
      " |          The data used to scale along the features axis.\n",
      " |      copy : bool, default=None\n",
      " |          Copy the input X or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      " |      and returns a transformed version of `X`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input samples.\n",
      " |      \n",
      " |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      " |          Target values (None for unsupervised transformations).\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# sklearn package\n",
    "import sklearn\n",
    "print(dir(sklearn))\n",
    "import sklearn.preprocessing\n",
    "print(dir(sklearn.preprocessing))\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "print(help(StandardScaler))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bd53fe",
   "metadata": {},
   "source": [
    "# 간단한 유형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f39513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398    5.0\n",
      "405    5.0\n",
      "400    5.6\n",
      "399    6.3\n",
      "414    7.0\n",
      "489    7.0\n",
      "401    7.2\n",
      "385    7.2\n",
      "415    7.2\n",
      "387    7.4\n",
      "Name: MEDV, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1. Top 10 구하기\n",
    "## 데이터 불러오기\n",
    "boston = pd.read_csv('./bigData/boston.csv')\n",
    "boston.head()\n",
    "\n",
    "## MEDV 데이터 오름차순으로 정렬 후 제일 작은 값 10개 출력 \n",
    "print(boston.sort_values(by='MEDV', ascending = True)['MEDV'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b346ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010595546094104624\n"
     ]
    }
   ],
   "source": [
    "# 2. 결측치 확인하기\n",
    "## 데이터 불러오기\n",
    "boston = pd.read_csv('./bigData/boston.csv')\n",
    "boston.head()\n",
    "\n",
    "## RM 칼럼의 결측값 개수\n",
    "sum(boston['RM'].isnull()) #15개\n",
    "\n",
    "## RM 칼럼의 평균값\n",
    "rm_mean = boston['RM'].mean() #6.285101832993899\n",
    "\n",
    "## 평균값으로 대체\n",
    "boston_rm = boston['RM'].fillna(rm_mean)\n",
    "\n",
    "## 결측치 삭제\n",
    "boston_drop = boston['RM'].dropna()\n",
    "\n",
    "print(abs(boston_rm.std() - boston_drop.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a5f2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3462.5\n"
     ]
    }
   ],
   "source": [
    "# 3. 이상값 확인하기\n",
    "## 데이터 확인\n",
    "boston = pd.read_csv('./bigData/boston.csv')\n",
    "boston.describe().T\n",
    "\n",
    "## ZN 칼럼의 평균값과 표준편차\n",
    "zn_mean = boston['ZN'].mean()\n",
    "zn_std = boston['ZN'].std()\n",
    "\n",
    "## 이상값 확인\n",
    "zn_lowest = zn_mean - 1.5*zn_std\n",
    "zn_highest = zn_mean + 1.5*zn_std\n",
    "print(sum(boston[boston['ZN'] < zn_lowest]['ZN']) + sum(boston[boston['ZN'] > zn_highest]['ZN']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86aaa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRIM         3.595038\n",
      "ZN          12.500000\n",
      "INDUS       12.910000\n",
      "NOX          0.175000\n",
      "RM           0.736000\n",
      "AGE         49.050000\n",
      "DIS          3.088250\n",
      "TAX        387.000000\n",
      "PTRATIO      2.800000\n",
      "B           20.847500\n",
      "LSTAT       10.005000\n",
      "MEDV         7.975000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 4. 사분위수 구하기\n",
    "## 데이터 확인\n",
    "boston = pd.read_csv('./bigData/boston.csv')\n",
    "new_boston = boston.drop(columns = ['CHAS', 'RAD'])\n",
    "new_boston.describe().T\n",
    "\n",
    "## IQR = Q3-Q1\n",
    "print(new_boston.describe().T['75%'] - new_boston.describe().T['25%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eee91b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.179644268774698 21.2 5.0 41.7\n"
     ]
    }
   ],
   "source": [
    "# 5. 순위 구하기\n",
    "## 데이터 확인\n",
    "boston = pd.read_csv('./bigData/boston.csv')\n",
    "\n",
    "## 30번째 큰값으로 1~29번째 큰값을 대체하고 평균, 중위수, 최솟값, 최댓값 구하기\n",
    "boston_medv = boston.sort_values(by = 'MEDV', ascending = False)['MEDV']\n",
    "boston_medv.iloc[:29] = boston_medv.iloc[29]\n",
    "print(boston_medv.mean(), boston_medv.median(), boston_medv.min(), boston_medv.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0337d691",
   "metadata": {},
   "source": [
    "# 복잡한 유형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8269a7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          COUNT\n",
      "CHAS RAD       \n",
      "0    1        3\n",
      "     2        2\n",
      "     3        5\n",
      "     4       33\n",
      "     5       51\n",
      "     6       17\n",
      "     24     124\n",
      "1    5        7\n",
      "     24       8\n"
     ]
    }
   ],
   "source": [
    "# 1. 그룹별 집계/요약\n",
    "## 데이터 확인\n",
    "boston = pd.read_csv('./bigData/boston.csv')\n",
    "\n",
    "## TAX 칼럼이 중앙값보다 큰 데이터만 활용\n",
    "new_boston = boston[boston['TAX'] > boston['TAX'].median()][['CHAS', 'RAD']]\n",
    "result = pd.DataFrame(new_boston.groupby(['CHAS', 'RAD'])['CHAS'].count())\n",
    "result.columns = ['COUNT']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53c99e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28490.5986459515\n"
     ]
    }
   ],
   "source": [
    "# 2. 오름차순/내림차순 정렬\n",
    "## 데이터 확인\n",
    "boston = pd.read_csv('./bigData/boston.csv')\n",
    "\n",
    "# TAX 칼럼 정렬 후 차이 구하기\n",
    "tax_asc = boston.sort_values(by = 'TAX', ascending = True, ignore_index= True)['TAX']\n",
    "tax_desc = boston.sort_values(by = 'TAX', ascending = False, ignore_index = True)['TAX']\n",
    "tax_diff = abs(tax_asc - tax_desc)\n",
    "print(tax_diff.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccfe98ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n"
     ]
    }
   ],
   "source": [
    "# 3. 최소최대 변환\n",
    "## 데이터 확인\n",
    "boston = pd.read_csv('./bigData/boston.csv')\n",
    "\n",
    "## MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "new_boston = pd.DataFrame(scaler.fit_transform(boston), columns = boston.columns)\n",
    "print(sum(new_boston['MEDV'] > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b6e582e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 43\n"
     ]
    }
   ],
   "source": [
    "# 4. 빈도값 구하기\n",
    "boston = pd.read_csv('./bigData/boston.csv')\n",
    "boston['AGE'] = round(boston['AGE'])\n",
    "age_mode = boston['AGE'].mode()[0];\n",
    "print(age_mode, sum(boston['AGE'] == age_mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46008ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n"
     ]
    }
   ],
   "source": [
    "# 5. 표준 변환\n",
    "boston = pd.read_csv('./bigData/boston.csv')\n",
    "\n",
    "## standard scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "new_boston = pd.DataFrame(scaler.fit_transform(boston), columns = boston.columns)\n",
    "dis = new_boston[new_boston['DIS'] > 0.4]['DIS']\n",
    "dis = dis[dis < 0.6]\n",
    "print(round(dis.mean(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79a6945e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3052\n"
     ]
    }
   ],
   "source": [
    "# 6. 유니크한 값\n",
    "boston = pd.read_csv('./bigData/boston.csv')\n",
    "sum = 0\n",
    "for i in boston.columns : \n",
    "    sum = sum + pd.DataFrame(boston[i].unique()).count()[0]\n",
    "print(sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
